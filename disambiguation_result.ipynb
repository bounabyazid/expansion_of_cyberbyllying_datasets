{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet\n",
    "from pywsd import disambiguate\n",
    "from pywsd.similarity import max_similarity as maxsim\n",
    "from nltk.wsd import lesk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 'love', Synset('love.n.04')), ('you', 'you', None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disambiguate('I went to the bank to deposit my money')\n",
    "disambiguate('love you', algorithm=maxsim, similarity_option='wup', keepLemmas=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'of', 'a', 'on'}\n"
     ]
    }
   ],
   "source": [
    "#stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "stop={'a', 'of', 'on', 'the'} #add related stop word here\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('datasets/only_cyberbyllying_formspring_datasets.csv')\n",
    "df_csv1 = pd.read_csv('datasets/not_cyberbyllying_formspring_datasets.csv')\n",
    "df_csv2 = pd.read_csv('datasets/ask_fm_data.csv')\n",
    "df_csv3 = pd.read_csv('datasets/negation_detected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>q_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fuck Justin bieber. He's a fucking fag</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>its also funny how u stalked my whole twitter!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Hello bitch! How's ur day going? Good I hope :...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Hey. Why you such a bitch?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>how old are u ? And how do you know michael?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>1021</td>\n",
       "      <td>would your rather cheat or be cheated on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>you might think im evil but u r the evil</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>1023</td>\n",
       "      <td>You're a bushwhacking  alchy piece of shit scrub.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>Your 44 years old? :O! Dumbass  Pedifile.:D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>if i told u den it would make it all the less ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1026 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                q_a  label\n",
       "0              0             Fuck Justin bieber. He's a fucking fag      1\n",
       "1              1  its also funny how u stalked my whole twitter!...      1\n",
       "2              2  Hello bitch! How's ur day going? Good I hope :...      1\n",
       "3              3                         Hey. Why you such a bitch?      1\n",
       "4              4       how old are u ? And how do you know michael?      1\n",
       "...          ...                                                ...    ...\n",
       "1021        1021          would your rather cheat or be cheated on?      0\n",
       "1022        1022           you might think im evil but u r the evil      1\n",
       "1023        1023  You're a bushwhacking  alchy piece of shit scrub.      1\n",
       "1024        1024        Your 44 years old? :O! Dumbass  Pedifile.:D      1\n",
       "1025        1025  if i told u den it would make it all the less ...      0\n",
       "\n",
       "[1026 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.head(10100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11024\n",
      "11024\n",
      "11024\n",
      "11024\n"
     ]
    }
   ],
   "source": [
    "#marge all data from files that has been read\n",
    "\n",
    "q_a=[]\n",
    "label=[]\n",
    "for i in range(len(df_csv)):\n",
    "    label.append(int(df_csv['label'][i]))\n",
    "    q_a.append(df_csv['q_a'][i])\n",
    "# print(len(q_a))\n",
    "# print(len(label))\n",
    "\n",
    "# for i in range(len(df_csv1)):\n",
    "#     label.append(int(df_csv1['label'][i]))\n",
    "#     q_a.append(df_csv1['q_a'][i])\n",
    "# print(len(q_a))\n",
    "# print(len(label))\n",
    "\n",
    "for i in range(len(df_csv2)):\n",
    "    label.append(int(df_csv2['label'][i]))\n",
    "    q_a.append(df_csv2['q_a'][i])\n",
    "print(len(q_a))\n",
    "print(len(label))\n",
    "    \n",
    "# for i in range(len(df_csv3)):\n",
    "#     label.append(int(df_csv3['label'][i]))\n",
    "#     q_a.append(df_csv3['q_a'][i])\n",
    "\n",
    "print(len(q_a))\n",
    "print(len(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11024\n"
     ]
    }
   ],
   "source": [
    "# text cleaning\n",
    "w=list(string.ascii_lowercase)\n",
    "QA=[]\n",
    "for i in q_a:\n",
    "    sen=i.replace('@USER','')\n",
    "    sen=sen.replace('@user','')\n",
    "    sen=sen.replace(\"#\", ' ')\n",
    "    sen=sen.replace(\"@\", '')\n",
    "    sen=sen.replace(\"pu**y\", 'pussy')\n",
    "    sen=sen.replace(\" mofo \", ' motherfucker')\n",
    "    sen=sen.replace(\"NEWLINE_TOKEN\", \" \")\n",
    "    sen=sen.replace(\"TAB_TOKEN\", \" \")\n",
    "    sen=sen.replace(\"??\", '?')\n",
    "    sen=sen.replace(\"? ?\", '?')\n",
    "    sen=sen.replace(\"??\", '?')\n",
    "    sen=sen.replace(\"? ?\", '?')\n",
    "    sen=sen.replace(\"? ?\", '?')\n",
    "    sen=sen.replace(\"? ?\", '?')\n",
    "    sen=sen.replace(\"?\", ' ? ')\n",
    "    sen=sen.replace(\"!\", ' ! ')\n",
    "    sen=sen.replace(\" y \", ' why ')\n",
    "    sen=sen.replace(\" u \", ' you ')\n",
    "    sen=sen.replace(\" w \", ' with ')\n",
    "    sen=sen.replace(\" mf \", ' motherfucker ')\n",
    "    sen=sen.replace(\" IDGAF \", ' i dont give a fuck ')\n",
    "    sen=sen.replace(\" IDRC \", ' I Dont Really Care ')\n",
    "    sen=sen.replace(\" tf \", ' the fuck ')\n",
    "    sen=sen.replace(\"wtf \", 'what the fuck ')\n",
    "    sen=sen.replace(\" yanno \", ' you know ')\n",
    "    sen=sen.replace(\" igt \", ' i know right ')\n",
    "    sen=sen.replace(\" r \", ' are ')\n",
    "    sen=sen.replace(\" tbh \", ' to be honest ')\n",
    "    sen=sen.replace(\" ik \", ' i know ')\n",
    "    sen=sen.replace(\" af \", ' as fuck ')\n",
    "    sen=sen.replace(\" hes \", ' he is ')\n",
    "    sen=sen.replace(\" des \", ' this ')\n",
    "    sen=sen.replace(\" bout \", ' about ')\n",
    "    sen=sen.replace(\" em \", ' them ')\n",
    "    sen=sen.replace(\" stg \", ' swear to god ')\n",
    "    sen=sen.replace(\" bj \", ' blow job ')\n",
    "    sen=sen.replace(\" ig \", ' i guess ')\n",
    "    sen=sen.replace(\" fagg \", ' faggot ')\n",
    "    sen=sen.replace(\" fag \", ' faggot ')\n",
    "    sen=sen.replace(\" fagot \", ' faggot ')\n",
    "    sen=sen.replace(\" tfw \", ' That Feel When ')\n",
    "    sen=sen.replace(\" wa \", ' was ')\n",
    "    sen=sen.replace(\" n \", ' and ')\n",
    "    sen=sen.replace(\" y0 \", ' you ')\n",
    "    sen=sen.replace(\" dat \", ' that ')\n",
    "    sen=sen.replace(\" yo \", ' you ')\n",
    "    sen=sen.replace(\"youre \", ' you are ')\n",
    "    sen=sen.replace(\" v \", ' vagina ')\n",
    "    sen=sen.replace(\" pusy \", ' pussy ')\n",
    "    sen=sen.replace(\" pusy\", ' pussy ')\n",
    "    sen=sen.replace(\"pussie\", 'pussy')\n",
    "    sen=sen.replace(\" v.\", ' vagina ')\n",
    "    sen=sen.replace(\" doggystyle \", ' doggy style ')\n",
    "    sen=sen.replace(\"di ck\", 'dick')\n",
    "    sen=sen.replace(\"r you\", 'are you')\n",
    "    sen=sen.replace(\"r u\", 'are you')\n",
    "    sen=sen.replace(\"f*ck\", 'fuck')\n",
    "    sen=sen.replace(\"f**\", 'fuck')\n",
    "    sen=sen.replace(\" fxk \", 'fuck')\n",
    "    sen=sen.replace(\" wil \", 'will')\n",
    "    sen=sen.replace(\" il \", 'i will')\n",
    "    sen=sen.replace(\" asss \", ' ass ')\n",
    "    sen=sen.replace(\" lifeles \", ' lifeless ')\n",
    "    sen=sen.replace(\" ugler \", ' uglier ')\n",
    "    sen=sen.replace(\" you're \", ' you are ') \n",
    "    sen=sen.replace(\",\", '.') \n",
    "    sen=sen.replace(\"?\", '.') \n",
    "    sen=sen.lower()\n",
    "    for s in w:\n",
    "        n=s+s+s\n",
    "        sen=sen.replace(n, s)\n",
    "    sen=\" \".join(sen.split())\n",
    "    QA.append(sen)\n",
    "# print(QA)\n",
    "q_a=QA\n",
    "print(len(q_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extention of emaning with pos tag\n",
    "def disambiguation(sen):\n",
    "    allWords=disambiguate(sen)\n",
    "#     print(allWords)\n",
    "    new_sen=[]\n",
    "    for x in allWords:\n",
    "        \n",
    "        \n",
    "        if x[0]=='fuck':\n",
    "            for l in x[1].lemmas():\n",
    "                if l.name() == 'fuck' or l.name() == 'fucking' or l.name() == 'screw' or l.name() == 'screwing' or l.name() == 'ass' or l.name() == 'nooky' or l.name() == 'nookie' or l.name() == 'piece_of_ass'  or l.name() == 'roll_in_the_hay' or l.name() == 'shag' or l.name() == 'shtup' or l.name() == 'sleep_together' or l.name() == 'roll_in_the_hay' or l.name() == 'make_out'  or l.name() == 'sleep_with' or l.name() == 'get_laid' or l.name() == 'have_sex'  or l.name() == 'be_intimate' or l.name() == 'have_intercourse'  or l.name() == 'screw' or l.name() == 'fuck' or l.name() == 'jazz' or l.name() == 'eff' or l.name() == 'hump' or l.name() == 'lie_with' or l.name() == 'bed'  or l.name() == 'bang'  or l.name() == 'bonk':\n",
    "                    ns=sen.replace(x[0], l.name())\n",
    "                    ns=ns.lower()\n",
    "                    new_sen.append(ns.replace('_', ' '))\n",
    "        elif x[0]=='love':\n",
    "            for l in x[1].lemmas():\n",
    "                if l.name()=='love' or l.name()=='passion' or l.name()=='beloved' or l.name()=='dearest' or l.name()=='honey' or l.name()=='know' or l.name()=='love_life' or l.name()=='have_a_go_at_it' or l.name()=='get_it_on':\n",
    "                    ns=sen.replace(x[0], l.name())\n",
    "                    ns=ns.lower()\n",
    "                    new_sen.append(ns.replace('_', ' '))\n",
    "        else:\n",
    "            try: \n",
    "                pos=x[1].pos()\n",
    "                for syn in wn.synsets(x[0]):\n",
    "                    if syn.pos()==pos:\n",
    "                        for l in syn.lemmas():\n",
    "                            ns=sen.replace(x[0], l.name())\n",
    "                            ns=ns.lower()\n",
    "                            new_sen.append(ns.replace('_', ' '))\n",
    "\n",
    "            except:\n",
    "                ns=sen.lower()\n",
    "                new_sen.append(ns.replace('_', ' '))\n",
    "    new_sen = list(dict.fromkeys(new_sen))\n",
    "    return new_sen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'PRP')\n",
      "('love', 'VBP')\n",
      "('her', 'PRP$')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['iodine love her',\n",
       " 'iodin love her',\n",
       " 'i love her',\n",
       " 'atomic number 53 love her',\n",
       " 'one love her',\n",
       " '1 love her',\n",
       " 'ace love her',\n",
       " 'single love her',\n",
       " 'unity love her',\n",
       " 'ane love her',\n",
       " 'i passion her',\n",
       " 'i beloved her',\n",
       " 'i dear her',\n",
       " 'i dearest her',\n",
       " 'i honey her',\n",
       " 'i sexual love her',\n",
       " 'i erotic love her',\n",
       " 'i lovemaking her',\n",
       " 'i making love her',\n",
       " 'i love life her',\n",
       " 'i enjoy her',\n",
       " 'i sleep together her',\n",
       " 'i roll in the hay her',\n",
       " 'i make out her',\n",
       " 'i make love her',\n",
       " 'i sleep with her',\n",
       " 'i get laid her',\n",
       " 'i have sex her',\n",
       " 'i know her',\n",
       " 'i do it her',\n",
       " 'i be intimate her',\n",
       " 'i have intercourse her',\n",
       " 'i have it away her',\n",
       " 'i have it off her',\n",
       " 'i screw her',\n",
       " 'i fuck her',\n",
       " 'i jazz her',\n",
       " 'i eff her',\n",
       " 'i hump her',\n",
       " 'i lie with her',\n",
       " 'i bed her',\n",
       " 'i have a go at it her',\n",
       " 'i bang her',\n",
       " 'i get it on her',\n",
       " 'i bonk her']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extention of emaning with all words\n",
    "def disambiguation(sen):\n",
    "    text = word_tokenize(sen)\n",
    "    allWords=nltk.pos_tag(text)\n",
    "    new_sen=[]\n",
    "    for x in allWords:\n",
    "        print(x)\n",
    "#         if x[0]=='fuck':\n",
    "#             synonyms = wn.synsets(x[0])\n",
    "#             for syn in synonyms:\n",
    "#                 for l in syn.lemmas():\n",
    "#                     if l.name() == 'fuck' or l.name() == 'fucking' or l.name() == 'screw' or l.name() == 'screwing' or l.name() == 'ass' or l.name() == 'nooky' or l.name() == 'nookie' or l.name() == 'piece_of_ass'  or l.name() == 'roll_in_the_hay' or l.name() == 'shag' or l.name() == 'shtup' or l.name() == 'sleep_together' or l.name() == 'roll_in_the_hay' or l.name() == 'make_out'  or l.name() == 'sleep_with' or l.name() == 'get_laid' or l.name() == 'have_sex'  or l.name() == 'be_intimate' or l.name() == 'have_intercourse'  or l.name() == 'screw' or l.name() == 'fuck' or l.name() == 'jazz' or l.name() == 'eff' or l.name() == 'hump' or l.name() == 'lie_with' or l.name() == 'bed'  or l.name() == 'bang'  or l.name() == 'bonk':\n",
    "#                         ns=sen.replace(x[0], l.name())\n",
    "#                         ns=ns.lower()\n",
    "#                         new_sen.append(ns.replace('_', ' '))\n",
    "                        \n",
    "#         elif x[0]=='love':\n",
    "#             synonyms = wn.synsets(x[0])\n",
    "#             for l in syn.lemmas():\n",
    "#                 for syn in synonyms:\n",
    "#                     for l in syn.lemmas():\n",
    "#                         if l.name()=='love' or l.name()=='passion' or l.name()=='beloved' or l.name()=='dearest' or l.name()=='honey' or l.name()=='know' or l.name()=='love_life' or l.name()=='have_a_go_at_it' or l.name()=='get_it_on':\n",
    "#                             ns=sen.replace(x[0], l.name())\n",
    "#                             ns=ns.lower()\n",
    "#                             new_sen.append(ns.replace('_', ' '))  \n",
    "\n",
    "        try: \n",
    "            synonyms = wn.synsets(x[0])\n",
    "            for syn in synonyms:\n",
    "                for l in syn.lemmas():\n",
    "                    ns=sen.replace(x[0], l.name())\n",
    "                    ns=ns.lower()\n",
    "                    new_sen.append(ns.replace('_', ' '))\n",
    "        except:\n",
    "            ns=sen.lower()\n",
    "            new_sen.append(ns.replace('_', ' '))\n",
    "    new_sen = list(dict.fromkeys(new_sen))\n",
    "    return new_sen\n",
    "disambiguation('I love her')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', None), ('hate', Synset('hate.v.01')), ('her', None)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i hate her', 'i detest her']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extention of meaning with disambiguation\n",
    "def disambiguation(sen):\n",
    "    allWords=disambiguate(sen)\n",
    "    print(allWords)\n",
    "    new_sen=[]\n",
    "    for x in allWords:\n",
    "        if x[0]=='fuck':\n",
    "            for l in x[1].lemmas():\n",
    "                if l.name() == 'fuck' or l.name() == 'fucking' or l.name() == 'screw' or l.name() == 'screwing' or l.name() == 'ass' or l.name() == 'nooky' or l.name() == 'nookie' or l.name() == 'piece_of_ass'  or l.name() == 'roll_in_the_hay' or l.name() == 'shag' or l.name() == 'shtup' or l.name() == 'sleep_together' or l.name() == 'roll_in_the_hay' or l.name() == 'make_out'  or l.name() == 'sleep_with' or l.name() == 'get_laid' or l.name() == 'have_sex'  or l.name() == 'be_intimate' or l.name() == 'have_intercourse'  or l.name() == 'screw' or l.name() == 'fuck' or l.name() == 'jazz' or l.name() == 'eff' or l.name() == 'hump' or l.name() == 'lie_with' or l.name() == 'bed'  or l.name() == 'bang'  or l.name() == 'bonk':\n",
    "                    ns=sen.replace(x[0], l.name())\n",
    "                    ns=ns.lower()\n",
    "                    new_sen.append(ns.replace('_', ' '))\n",
    "        elif x[0]=='love':\n",
    "            for l in x[1].lemmas():\n",
    "                if l.name()=='love' or l.name()=='passion' or l.name()=='beloved' or l.name()=='dearest' or l.name()=='honey' or l.name()=='know' or l.name()=='love_life' or l.name()=='have_a_go_at_it' or l.name()=='get_it_on':\n",
    "                    ns=sen.replace(x[0], l.name())\n",
    "                    ns=ns.lower()\n",
    "                    new_sen.append(ns.replace('_', ' '))\n",
    "        else:\n",
    "            try: \n",
    "                for l in x[1].lemmas():\n",
    "                        ns=sen.replace(x[0], l.name())\n",
    "                        ns=ns.lower()\n",
    "                        new_sen.append(ns.replace('_', ' '))\n",
    "            except:\n",
    "                ns=sen.lower()\n",
    "                new_sen.append(ns.replace('_', ' '))\n",
    "        new_sen = list(dict.fromkeys(new_sen))\n",
    "    return new_sen\n",
    "\n",
    "disambiguation('I hate her')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_q_a=[]\n",
    "new_label=[]\n",
    "for i in  range(len(q_a)):   \n",
    "    collction_sen= disambiguation(q_a[i])\n",
    "    for x in collction_sen:\n",
    "        new_q_a.append(x)\n",
    "        new_label.append(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_meanings(word):\n",
    "#     synonyms = []\n",
    "#     # Get all possible meanings of the word\n",
    "#     for syn in wn.synsets(word):\n",
    "#         print(syn)\n",
    "#         for l in syn.lemmas():\n",
    "#             print(l)\n",
    "#             synonyms.append(l.name())\n",
    "#     return synonyms\n",
    "# word = input(\"type a word to retrieve all other words with similar meaning: \")\n",
    "# synoym = extract_meanings(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618155\n",
      "618155\n"
     ]
    }
   ],
   "source": [
    "print(len(new_q_a))\n",
    "print(len(new_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = { 'q_a':new_q_a,'label':new_label}\n",
    "df = pd.DataFrame(my_dict)\n",
    "# df.to_csv('expansation_semantinc_meaning_using_postag',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files\n",
    "comments_attack=pd.DataFrame()\n",
    "comments_attack['comment']=new_q_a\n",
    "comments_attack['label']=new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = comments_attack\n",
    "\n",
    "# Text preprocessing\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "dataframe['comment'] = dataframe['comment'].apply(lambda x: x.lower())\n",
    "# dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('[/(){}\\[\\]\\|@,;]','',x)))\n",
    "# dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub('[^0-9a-z #+_]','',x)))\n",
    "# dataframe['comment'] = dataframe['comment'].apply((lambda x: re.sub(' +',' ',x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-a86fd14956a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#this code will remove stop word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#stop word removing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "#this code will remove stop word\n",
    "for c in stop: #stop word removing\n",
    "    dataframe['comment'] = dataframe['comment'].apply(lambda x: x.replace(' '+c+' ', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "         ..\n",
      "618150    0\n",
      "618151    0\n",
      "618152    0\n",
      "618153    0\n",
      "618154    0\n",
      "Name: label, Length: 618155, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 fuck justin bieber. he's a fucking fag\n",
      "1           fucking justin bieber. he's a fuckinging fag\n",
      "2               screw justin bieber. he's a screwing fag\n",
      "3         screwing justin bieber. he's a screwinging fag\n",
      "4                   ass justin bieber. he's a assing fag\n",
      "                               ...                      \n",
      "618150                           hunting watch hayes omg\n",
      "618151                            hunter helen hayes omg\n",
      "618152                    hunter rutherford b. hayes omg\n",
      "618153              hunter rutherford birchard hayes omg\n",
      "618154                        hunter president hayes omg\n",
      "Name: comment, Length: 618155, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(dataframe['comment'], dataframe['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train comments length:  494524\n",
      "test comments length:  123631\n"
     ]
    }
   ],
   "source": [
    "print('train comments length: ',len(train_x))\n",
    "print('test comments length: ',len(valid_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'.')\n",
    "count_vect.fit(dataframe['comment'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(dataframe['comment'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# word level ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,4), max_features=5000)\n",
    "tfidf_vect_ngram.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(dataframe['comment'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)\n",
    "\n",
    "#feartures nlp\n",
    "# dataframe['char_count'] = dataframe['comment'].apply(len)\n",
    "# dataframe['word_count'] = dataframe['comment'].apply(lambda x: len(x.split()))\n",
    "# dataframe['word_density'] = dataframe['char_count'] / (dataframe['word_count']+1)\n",
    "# dataframe['punctuation_count'] = dataframe['comment'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "# dataframe['title_word_count'] = dataframe['comment'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "# dataframe['upper_case_word_count'] = dataframe['comment'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(dataframe['comment'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure e\n",
    "# qual length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, xtrain, ytrain, xvalid, yvalid): \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(xtrain, ytrain)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(xvalid)     \n",
    "    accuracy = metrics.accuracy_score(predictions, yvalid)\n",
    "    f1score = metrics.f1_score(yvalid, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF:   accuracy: 0.9058165023335571     f1 score: 0.8955371431323509\n",
      "NB, N-Gram Vectors:   accuracy: 0.830002183918273     f1 score: 0.8023529375402864\n",
      "NB, CharLevel Vectors:   accuracy: 0.8797469890237886   f1 score: 0.8616585232974142\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "# accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "# print(\"NB, Count Vectors:   accuracy: %s      f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjahan18\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:   accuracy: 0.9586026158487758   f1 score: 0.9580267970325165\n",
      "LR, N-Gram Vectors:   accuracy: 0.8471257208952447   f1 score: 0.8104175998950054\n",
      "LR, CharLevel Vectors:   accuracy: 0.9576562512638416   f1 score: 0.9569269222435989\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "# accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "# print(\"LR, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"LR, N-Gram Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(train_y, 3)\n",
    "valid_y_onehot = keras.utils.to_categorical(valid_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, epochs = 3):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(3, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=epochs)\n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "494524/494524 [==============================] - 930s 2ms/step - loss: 0.1305 - accuracy: 0.9508\n",
      "Epoch 2/3\n",
      "494524/494524 [==============================] - 931s 2ms/step - loss: 0.0454 - accuracy: 0.9838\n",
      "Epoch 3/3\n",
      "494524/494524 [==============================] - 1156s 2ms/step - loss: 0.0345 - accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(3, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score = metrics.f1_score(valid_y, predictions, average='weighted')\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(i)  Load each Sentance\n",
    "(ii) Perform word tokenize, make a list of words\n",
    "(ii) FOR each word, disambiguate and find sense specific Synset\n",
    "        IF sense sense spefic Synset has Synonyms\n",
    "            FOR each synonym\n",
    "                Replace sentance word with  synonym and generate new sentance\n",
    "            END FOR\n",
    "        END IF\n",
    "    END FOR\n",
    "    END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
